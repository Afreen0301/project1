{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11580452,"sourceType":"datasetVersion","datasetId":7261007}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Marge + split dataset","metadata":{}},{"cell_type":"code","source":"import os\nimport shutil\nimport random\n\n# Paths\nsource_dir = '/kaggle/working/merged_dataset'  # merged folder\ndest_dir = '/kaggle/working/merged_dataset_split'  # new split folder\nclasses = ['Benign', 'Malignant']\n\n# Create folders\nfor split in ['train', 'val', 'test']:\n    for cls in classes:\n        os.makedirs(os.path.join(dest_dir, split, cls), exist_ok=True)\n\n# Split ratio\ntrain_ratio = 0.7\nval_ratio = 0.15\ntest_ratio = 0.15\n\n# Split images\nfor cls in classes:\n    cls_path = os.path.join(source_dir, cls)\n    images = os.listdir(cls_path)\n    random.shuffle(images)\n\n    total = len(images)\n    train_end = int(total * train_ratio)\n    val_end = train_end + int(total * val_ratio)\n\n    train_images = images[:train_end]\n    val_images = images[train_end:val_end]\n    test_images = images[val_end:]\n\n    # Copy images\n    for img in train_images:\n        shutil.copy(os.path.join(cls_path, img), os.path.join(dest_dir, 'train', cls))\n    for img in val_images:\n        shutil.copy(os.path.join(cls_path, img), os.path.join(dest_dir, 'val', cls))\n    for img in test_images:\n        shutil.copy(os.path.join(cls_path, img), os.path.join(dest_dir, 'test', cls))\n\nprint(\"✅ Dataset successfully split into Train, Validation, Test!\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Overview of Dataset + Display train images","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport random\nimport cv2\n\n# === Set your split base directory ===\nsplit_base_dir = '/kaggle/working/merged_dataset_split'  # <-- your split folder here\n\n# Step 1: Bar Chart of counts\nsplit_dirs = ['train', 'val', 'test']\nclasses = ['Benign', 'Malignant']\n\ncounts = {split: [] for split in split_dirs}\n\nfor split in split_dirs:\n    for cls in classes:\n        path = os.path.join(split_base_dir, split, cls)\n        counts[split].append(len(os.listdir(path)))\n\n# Plot bar chart\nx = np.arange(len(classes))  # Label locations\nwidth = 0.25  # Width of the bars\n\nfig, ax = plt.subplots(figsize=(10,6))\nrects1 = ax.bar(x - width, counts['train'], width, label='Train')\nrects2 = ax.bar(x, counts['val'], width, label='Validation')\nrects3 = ax.bar(x + width, counts['test'], width, label='Test')\n\n# Add text labels and title\nax.set_ylabel('Number of Images')\nax.set_title('Number of Images per Class and Split')\nax.set_xticks(x)\nax.set_xticklabels(classes)\nax.legend()\n\n# Add counts on top of bars\nfor rect in rects1 + rects2 + rects3:\n    height = rect.get_height()\n    ax.annotate(f'{height}', xy=(rect.get_x() + rect.get_width() / 2, height),\n                xytext=(0, 3), textcoords=\"offset points\",\n                ha='center', va='bottom')\n\nplt.show()\n\n# ---------------------------------------------\n# Step 2: Randomly show 5 train images from each class\n\ndef show_random_images(base_dir, classes, n_images=5):\n    fig, axes = plt.subplots(2, n_images, figsize=(n_images*3, 6))\n    fig.suptitle('Random Train Images: Benign vs Malignant', fontsize=16)\n\n    for i, cls in enumerate(classes):\n        cls_dir = os.path.join(base_dir, 'train', cls)\n        images = os.listdir(cls_dir)\n        selected_imgs = random.sample(images, n_images)\n\n        for j, img_name in enumerate(selected_imgs):\n            img_path = os.path.join(cls_dir, img_name)\n            img = cv2.imread(img_path)\n            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n            axes[i, j].imshow(img)\n            axes[i, j].axis('off')\n            axes[i, j].set_title(cls)\n\n    plt.tight_layout()\n    plt.show()\n\n# Display random train images\nshow_random_images(split_base_dir, classes)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Preprocessing","metadata":{}},{"cell_type":"markdown","source":"# Display Preprocessed images","metadata":{}},{"cell_type":"markdown","source":"# Augmentation","metadata":{}},{"cell_type":"markdown","source":"# Display Augmented Images","metadata":{}},{"cell_type":"markdown","source":"# ViT Feature Extractor","metadata":{}},{"cell_type":"markdown","source":"# Train Model+Evalution+ Visualization","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\nfrom sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, roc_curve, f1_score, accuracy_score, precision_score, recall_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neural_network import MLPClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import VotingClassifier\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport joblib\n\n# Load extracted ViT features and labels\nX_vit = np.load('/kaggle/working/X_vit_features.npy')\ny_vit = np.load('/kaggle/working/y_vit_labels.npy')\n\n# Check shapes and balance of dataset\nprint(f\"X_vit shape: {X_vit.shape}, y_vit shape: {y_vit.shape}\")\nprint(f\"Unique labels in y_vit: {np.unique(y_vit, return_counts=True)}\")\n\n# Split data (with stratify and fixed seed)\nX_train, X_test, y_train, y_test = train_test_split(\n    X_vit, y_vit, test_size=0.2, random_state=42, stratify=y_vit\n)\n\n# Save test set for future evaluation (optional)\nnp.save('/kaggle/working/X_test.npy', X_test)\nnp.save('/kaggle/working/y_test.npy', y_test)\n\n# Define Logistic Regression grid with class weights for imbalance handling\nlr_params = {\n    'C': [0.01, 0.1, 1, 10],\n    'penalty': ['l2'],\n    'solver': ['lbfgs', 'liblinear'],\n    'class_weight': ['balanced', None]  # Added class weight to handle imbalance\n}\nlr_grid = GridSearchCV(LogisticRegression(max_iter=1000), lr_params, cv=3, scoring='accuracy')\nlr_grid.fit(X_train, y_train)\nprint(\"✅ Best Logistic Regression Params:\", lr_grid.best_params_)\n\n# Define XGBoost random grid with class weights for imbalance handling\nxgb_params = {\n    'n_estimators': [50, 100, 150],\n    'max_depth': [3, 5, 7],\n    'learning_rate': [0.01, 0.05, 0.1],\n    'subsample': [0.7, 0.8, 1.0],\n    'colsample_bytree': [0.7, 0.8, 1.0],\n    'scale_pos_weight': [1, 2, 5]  # For handling class imbalance in XGBoost\n}\nxgb_grid = RandomizedSearchCV(\n    XGBClassifier(use_label_encoder=False, eval_metric='logloss'),\n    xgb_params, cv=3, n_iter=10, scoring='accuracy', random_state=42\n)\nxgb_grid.fit(X_train, y_train)\nprint(\"✅ Best XGBoost Params:\", xgb_grid.best_params_)\n\n# Define MLP random grid with class weights for imbalance handling\nmlp_params = {\n    'hidden_layer_sizes': [(128,), (256,), (256, 128)],\n    'activation': ['relu', 'tanh'],\n    'solver': ['adam', 'sgd'],\n    'alpha': [0.0001, 0.001],\n    'class_weight': ['balanced', None],\n    'max_iter': [200, 500]\n}\nmlp_grid = RandomizedSearchCV(MLPClassifier(), mlp_params, cv=3, n_iter=10, scoring='accuracy', random_state=42)\nmlp_grid.fit(X_train, y_train)\nprint(\"✅ Best MLP Params:\", mlp_grid.best_params_)\n\n# Create the ensemble model with VotingClassifier\nensemble_model = VotingClassifier(\n    estimators=[\n        ('lr', lr_grid.best_estimator_),\n        ('mlp', mlp_grid.best_estimator_),\n        ('xgb', xgb_grid.best_estimator_)\n    ], voting='hard'\n)\n\n# Train ensemble model\nensemble_model.fit(X_train, y_train)\n\n# Evaluate ensemble model\ny_pred = ensemble_model.predict(X_test)\ny_proba = ensemble_model.predict_proba(X_test)[:, 1]  # Probability for ROC/AUC\n\n# Confusion Matrix\nconf_matrix = confusion_matrix(y_test, y_pred)\nsns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Benign', 'Malignant'], yticklabels=['Benign', 'Malignant'])\nplt.title('Confusion Matrix')\nplt.ylabel('Actual')\nplt.xlabel('Predicted')\nplt.show()\n\n# Classification Report\nprint(\"Classification Report:\")\nprint(classification_report(y_test, y_pred))\n\n# Accuracy Score\nprint(f\"Accuracy: {accuracy_score(y_test, y_pred)}\")\n\n# F1 Score\nprint(f\"F1 Score: {f1_score(y_test, y_pred)}\")\n\n# ROC Curve & AUC Score\nfpr, tpr, thresholds = roc_curve(y_test, y_proba)\nroc_auc = roc_auc_score(y_test, y_proba)\nplt.figure(figsize=(10, 6))\nplt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic (ROC)')\nplt.legend(loc=\"lower right\")\nplt.show()\n\n# Save the trained ensemble model for future use\njoblib.dump(ensemble_model, '/kaggle/working/ensemble_model.joblib')\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}